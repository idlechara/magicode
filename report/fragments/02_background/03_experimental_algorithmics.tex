\section{Experimental algorithmics}
\label{SECTION:EXPERIMENTAL_ALGORITHMICS}

Experimental algorithmics (EA) can be seen as a spin-off\footnote{We use the term ``spin-off'' as we are emphasizing the fact that it is a DoE process directly applied to a field which relies heavily on exact mathematical analysis to give results. } of \textit{Design of Experiments} (DoE). DoE are a collection of statistic techniques applied to understand how a process is affected by the relationships between its variables and external factors, by systematically studying and explaining the variation of information provided by altering the environment of the same process~\cite{Wagner_Mount_Giles_2014}.\\

Mostly in algorithm design, pure mathematical and theoretical approaches are taken in order to devise how to create, develop, and optimize new algorithms. However, the gap between theoretical analysis and the actual implementation is still huge with the rise of new platforms and compute architectures nowadays.\\

In the case of computer science, computational experiments by any means do not try to replace theoretical analysis, but rather to complement and speed up the discovery process by guiding their research, tuning, and cyclical implementation via empirical results.\\

The most complete compilation of techniques and recommendations on how to apply design of experiments to algorithm design is currently made by Catherine McGeoch on her book \textit{A guide to experimental Algorithmics}~\cite{10.5555/2159557}, on which she introduces a compressive guide on how to apply this method by introducing a fair amount of guidelines for computer scientists.\\

\subsection{Methodology foundations}

\subsubsection{Algorithm instantiation hierarchy}

On experimental algorithmics there is no such difference between algorithms and programs as programmers usually use\footnote{For developers, algorithms are usually referred as an abstract blueprint for describing a process while the program is the executable instance usually without any middle ground in it. Concepts like processes or the intermediate products of their implementation tend to be ignored as they are part of the previous aforementioned definitions.}. Instead, the differences on the abstraction level introduced as how specific is their representation on a target environment. Driven by this, we can divide our scale of instantiation as follows:\\

\begin{itemize}
    \item \textit{Metaheuristics and algorithm paradigms}, describing algorithmic structures which are not needed to be tied to a specific problem or domain at hand. For example, \textit{Dijkstra's algorithm}~\cite{10.5555/1614191} is a \emph{dynamic programming} algorithm while \textit{2-opt} is a \emph{metaheuristic}~\cite{10.2307/167074}. As such, we can consider them as blueprints to solve generic problems.
    \item \textit{Algorithms}, being as the description step-by-step of the process for solving a specific instance of a problem. In our case, the pseudocode used to describe \textit{MinMax} belongs to this level. Note that since at this point we have more information on what is happening, we can add more details as needed or desired.
    \item \textit{Source program} as the implementation of the algorithm in a particular high-level language intended to be deployed on a given environment. At this level, the specification is given by the language standards and conventions, as at source code level there is no machine-specific code, the target platform is not relevant at this point.
    \item \textit{Object code} as the result of the compilation (or execution) process of the source code, being influenced by the architecture, environment and machine specs.
    \item \textit{Process} is the highest level of representation as it is an active program running on a particular machine at a particular moment. At this level, any metric can be affected by both internal or external conditions such as currently running processes, shape of memory, electronics, etc.
\end{itemize}

\subsubsection{Algorithm design hierarchy}

One of the main goals of algorithm engineering is to combine results from different instantiation levels and strategies in order to overcome the roadblocks generated by the gap present between theory and experience. In order to understand this, we need to divide the algorithm design process as a linear hierarchy composed by six elements, namely:\\

\begin{itemize}
    \item \textit{System structure} is the decomposition of the software into modules that present an efficient interaction layer between themselves. At this point, the developer needs to check the target runtime, sequential or parallel processing, and environment support.
    \item \textit{Algorithm and data structure design} specify the exact problem that is being solved on each module and decomposes its implementation (useful for class-oriented languages). At this point, the developer should take care on selecting algorithms and data structures that are asymptotically efficient and that offer an accurate problem representation.
    \item \textit{Implementation and algorithm tuning}, or maybe building and tuning a family of them depending on the goals of the study. Timing can be performed  at high level structures in relation to its paradigm but can also be tuned from the input or cost computation model, depending on which problem is being solved.
    \item \textit{Code tuning} being performed at low-level code-specific properties, ranging from procedural calls, loops, memory management, etc. When performing code tuning, transformations to the base code must be made systematically in order to get an equivalent program with better performance.
    \item \textit{System software} can also affect the performance. To help alleviate this, tweaks to the runtime environment related elements ---like compilers--- can help to improve performance on certain cases.
    \item \textit{Platform and hardware} being the last instance of tuning, by moving the implementation to another architecture, another CPU, or adding coprocessors by example.
\end{itemize}

\subsection{Considerations}
\label{SUBSECTION:EXPERIMENTAL_ALGORITMICS_CONSIDERATIONS}

It is important to always take into account all levels of algorithm design and instantiation hierarchies. Usually in academia, algorithm designers tend to focus only on the first two levels of the algorithm instantiation hierarchy, completely ignoring the remaining three ones, which could lead on algorithms with attractive asymptotic bounds but with worse than desirable performance in practice.\\

But as important of taking into account of all level of design, it's also important to remark that this process is not linear. It is bound that some changes can present new opportunities, roadblocks, paradigm changes and so on so forth, and when it happens it may be needed to start from scratch or to skip some steps depending on the situation.\\


\subsection{Methodology overview}

On experimental algorithmics, the first step is to plan the involved experiments according to the following steps in a cyclic way:\\

\subsubsection{Planning}
\begin{itemize}
    \item Formulate a question.
    \item Assemble or build the test environment.
    \item Experiment design to address the question.
\end{itemize}

At this stage, we do not analyze any data yet, as we only design the process to study at a later stage. Given that the experimental setup can alter the question at hand, this process tends to repeat until the experiment is fully assembled.\\

In order to determine if a given experiment is viable --- namely, a \emph{workhorse experiment} ---, the practitioner must perform a series of \emph{pilot experiments} beforehand, in order to understand the environment, implementation challenges and the problem itself. \\

Pilot experiments are expected to be small experiments which answer a highly constrained and specific question that drives towards the construction of the workhorse experiment, as workhorse experiments are expected to be complex in both setup, execution, and analysis. \\

At this step, it is expected that the practitioner develops metrics, indicators, and configurations which leads to a deeper understanding of the original question proposed.\\

\subsubsection{Execution}
Whilst this step is taken locally as a sequential process, there is a mutual dependency between the execution step and the planning stages. The execution of both pilot and workhorse experiments generates data which is used to gain information and insight on the context. But at the same time, in order to set the context, a set of previous results are needed. \\

Because of this mutual dependency between those two processes, it is agreed that if the question at hand is answered then the process is finished. Otherwise, the process starts again from the planning stage taking the results from this stage as input.\\

\subsection{Result driven development}
As results from experiment execution and analysis yield data from all the levels of algorithm design, those results are used to tune up existing algorithms in order to optimize their execution for certain cases, specific architectures, or given constraints.\\

One of the key benefits of this strategy is that whilst a pure theoretical approach can be difficult or even being infeasible in some cases, a systematic experimental approach can help to both guide theoretical analysis by giving insight and validating theoretical results.\\

This approach is widely used on metaheuristics tuning, as there is no guarantee of returning optimal solutions or even worse, converging into one. Experiments are used to fill the gap by simplifying assumptions necessary to theory and the realistic use cases in a \emph{nuts-and-bolts} fashion\footnote{By randomly turning dials and pressing switches in a machine until something happens}. \\

Characterizing and improve profiling, gain insight on average and worst cases, suggest new theorems and proof strategies, and to extend theoretical analyses to realistic inputs becomes part of this cycle driven by a necessity of replicate the effects at a later time.\\

An example of the use of this methodology is the building of \textit{LZ-index}~\cite{Navarro_2009}, a compressed data structure designed to support indexing and fast lookup. Navarro used experiments to guide the choices during the implementation process and compare the finished product against current competing strategies.\\
