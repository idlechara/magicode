
\section{Rationale}
IncrementalQuickSort and its introspective version, IntrospectiveQuickSort, already have their theoretical analyses for the worst case instances. But such theoretical analysis is not always feasible, sometimes not easy and most of the time it is not realistic. As a practical example of it, when testing IQS against HeapSort for a full array sorting under architectures with small cache memory, IQS outperforms vastly HeapSort as it trashes the cache on each iteration. But when cache units are large enough to support the entire array, there is no point on using IQS, as most operations are actually solved on cache directly. There is a huge gap when it comes to practice on algorithm design, and IIQS is also not free of such problems.\\

The main issue arose when it comes to the analysis of the median-of-medians effects on the partition. As the execution of this algorithm offsets itself on each partition, it is the equivalent to run continuously a process which reduces the overall disorder of the sequence\footnote{We do not talk explicitly of any disorder metric seen in Section~\ref{SEC:MEASURING_DISORDER} as the effect depends on the process behind each execution. In this sense, we want our definition to be abstract and not to be tied with any algorithm implementation.}.\\

This effect displaces the elements on the sequence towards their expected position on it, due to the adaptive-sorting nature of both IQS and IIQS makes the overall running time dependent on the element distribution. This makes really hard the use of standard techniques like amortized analysis to study the behavior of IIQS. Even worse, due to the increased complexity of the algorithm, its theoretical analysis is likely to differ from the practical results.\\

The problem at hand is to modify the current implementation of IQS and IIQS to support an extra case, which is when the sequence of elements can have repeated elements on it. This is now a problem for many reasons as it messes up the pivot selection heuristics and partition stages of the original algorithm.\\

Due to the aforementioned reasons, we want to take an experimental approach to analyze this new instance and use the results to guide the development of an extension of this algorithm.\\

\FloatBarrier